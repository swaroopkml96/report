@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
@article{iandola2014densenet,
  title={Densenet: Implementing efficient convnet descriptor pyramids},
  author={Iandola, Forrest and Moskewicz, Matt and Karayev, Sergey and Girshick, Ross and Darrell, Trevor and Keutzer, Kurt},
  journal={arXiv preprint arXiv:1404.1869},
  year={2014}
}
@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick and others},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Taipei, Taiwan}
}
@article{Vajda2018,
author = {Vajda, Szil{\'{a}}rd and Karargyris, Alexandros and Jaeger, Stefan and Santosh, K.C. and Candemir, Sema and Xue, Zhiyun and Antani, Sameer and Thoma, George},
doi = {10.1007/s10916-018-0991-9},
issn = {0148-5598},
journal = {Journal of Medical Systems},
month = {aug},
number = {8},
pages = {146},
publisher = {Springer US},
title = {{Feature Selection for Automatic Tuberculosis Screening in Frontal Chest Radiographs}},
url = {http://link.springer.com/10.1007/s10916-018-0991-9},
volume = {42},
year = {2018}
}
@incollection{Liu2018,
author = {Liu, Junyu and Liu, Yang and Wang, Cheng and Li, Anwei and Meng, Bowen and Chai, Xiangfei and Zuo, Panli},
doi = {10.1007/978-3-030-01421-6_16},
month = {oct},
pages = {158--166},
publisher = {Springer, Cham},
title = {{An Original Neural Network for Pulmonary Tuberculosis Diagnosis in Radiographs}},
url = {http://link.springer.com/10.1007/978-3-030-01421-6{\_}16},
year = {2018}
}
@article{Jaeger2014,
abstract = {Tuberculosis is a major health threat in many regions of the world. Opportunistic infections in immunocompromised HIV/AIDS patients and multi-drug-resistant bacterial strains have exacerbated the problem, while diagnosing tuberculosis still remains a challenge. When left undiagnosed and thus untreated, mortality rates of patients with tuberculosis are high. Standard diagnostics still rely on methods developed in the last century. They are slow and often unreliable. In an effort to reduce the burden of the disease, this paper presents our automated approach for detecting tuberculosis in conventional posteroanterior chest radiographs. We first extract the lung region using a graph cut segmentation method. For this lung region, we compute a set of texture and shape features, which enable the X-rays to be classified as normal or abnormal using a binary classifier. We measure the performance of our system on two datasets: a set collected by the tuberculosis control program of our local county's health department in the United States, and a set collected by Shenzhen Hospital, China. The proposed computer-aided diagnostic system for TB screening, which is ready for field deployment, achieves a performance that approaches the performance of human experts. We achieve an area under the ROC curve (AUC) of 87{\%} (78.3{\%} accuracy) for the first set, and an AUC of 90{\%} (84{\%} accuracy) for the second set. For the first set, we compare our system performance with the performance of radiologists. When trying not to miss any positive cases, radiologists achieve an accuracy of about 82{\%} on this set, and their false positive rate is about half of our system's rate.},
author = {Jaeger, Stefan and Karargyris, Alexandros and Candemir, Sema and Folio, Les and Siegelman, Jenifer and Callaghan, Fiona and {Zhiyun Xue} and Palaniappan, Kannappan and Singh, Rahul K. and Antani, Sameer and Thoma, George and {Yi-Xiang Wang} and {Pu-Xuan Lu} and McDonald, Clement J.},
doi = {10.1109/TMI.2013.2284099},
issn = {0278-0062},
journal = {IEEE Transactions on Medical Imaging},
month = {feb},
number = {2},
pages = {233--245},
pmid = {24108713},
title = {{Automatic Tuberculosis Screening Using Chest Radiographs}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24108713 http://ieeexplore.ieee.org/document/6616679/},
volume = {33},
year = {2014}
}
@inproceedings{Hwang2016,
abstract = {Tuberculosis (TB) is one of the major global health threats especially in developing countries. Although newly diagnosed TB patients can be recovered with high cure rate, many curable TB patients in the developing countries are obliged to die because of delayed diagnosis, partly by the lack of radiography and radiologists. Therefore, developing a computer-aided diagnosis (CAD) system for TB screening can contribute to early diagnosis of TB, which results in prevention of deaths from TB. Currently, most CAD algorithms adopt carefully designed morphological features distinguishing different lesion types to improve screening performances. However, such engineered features cannot be guaranteed to be the best descriptors for TB screening. Deep learning has become a majority in machine learning society. Especially in computer vision fields, it has been verified that deep convolutional neural networks (CNN) is a very promising algorithm for various visual tasks. Since deep CNN enables end-to-end training from feature extraction to classification, it does not require objective-specific manual feature engineering. In this work, we designed CAD system based on deep CNN for automatic TB screening. Based on large-scale chest X-rays (CXRs), we achieved viable TB screening performance of 0.96, 0.93 and 0.88 in terms of AUC for three real field datasets, respectively, by exploiting the effect of transfer learning.},
author = {Hwang, Sangheum and Kim, Hyo-Eun and Jeong, Jihoon and Kim, Hee-Jin},
doi = {10.1117/12.2216198},
editor = {Tourassi, Georgia D. and Armato, Samuel G.},
keywords = {Deep convolutional neural networks,TB CAD,TB screening,Transfer learning,Tuberculosis CAD,Tuberculosis screening},
month = {mar},
pages = {97852W},
publisher = {International Society for Optics and Photonics},
title = {{A novel approach for tuberculosis screening based on deep convolutional neural networks}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2216198},
volume = {9785},
year = {2016}
}
@article{TheDeepRadiologyTeam2018,
abstract = {In this work, we describe our approach to pneumonia classification and localization in chest radiographs. This method uses only $\backslash$emph{\{}open-source{\}} deep learning object detection and is based on CoupleNet, a fully convolutional network which incorporates global and local features for object detection. Our approach achieves robustness through critical modifications of the training process and a novel ensembling algorithm which merges bounding boxes from several models. We tested our detection algorithm tested on a dataset of 3000 chest radiographs as part of the 2018 RSNA Pneumonia Challenge; our solution was recognized as a winning entry in a contest which attracted more than 1400 participants worldwide.},
archivePrefix = {arXiv},
arxivId = {1811.08939},
author = {{The DeepRadiology Team}, The DeepRadiology},
eprint = {1811.08939},
file = {:home/administrator/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/The DeepRadiology Team - 2018 - Pneumonia Detection in Chest Radiographs.pdf:pdf},
month = {nov},
title = {{Pneumonia Detection in Chest Radiographs}},
url = {http://arxiv.org/abs/1811.08939},
year = {2018}
}
@article{Wang2017,
abstract = {The chest X-ray is one of the most commonly accessible radiological examinations for screening and diagnosis of many lung diseases. A tremendous number of X-ray imaging studies accompanied by radiological reports are accumulated and stored in many modern hospitals' Picture Archiving and Communication Systems (PACS). On the other side, it is still an open question how this type of hospital-size knowledge database containing invaluable imaging informatics (i.e., loosely labeled) can be used to facilitate the data-hungry deep learning paradigms in building truly large-scale high precision computer-aided diagnosis (CAD) systems. In this paper, we present a new chest X-ray database, namely "ChestX-ray8", which comprises 108,948 frontal-view X-ray images of 32,717 unique patients with the text-mined eight disease image labels (where each image can have multi-labels), from the associated radiological reports using natural language processing. Importantly, we demonstrate that these commonly occurring thoracic diseases can be detected and even spatially-located via a unified weakly-supervised multi-label image classification and disease localization framework, which is validated using our proposed dataset. Although the initial quantitative results are promising as reported, deep convolutional neural network based "reading chest X-rays" (i.e., recognizing and locating the common disease patterns trained with only image-level labels) remains a strenuous task for fully-automated high precision CAD systems. Data download link: https://nihcc.app.box.com/v/ChestXray-NIHCC},
archivePrefix = {arXiv},
arxivId = {1705.02315},
author = {Wang, Xiaosong and Peng, Yifan and Lu, Le and Lu, Zhiyong and Bagheri, Mohammadhadi and Summers, Ronald M.},
doi = {10.1109/CVPR.2017.369},
eprint = {1705.02315},
file = {:home/administrator/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2017 - ChestX-ray8 Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization.pdf:pdf},
month = {may},
title = {{ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases}},
url = {http://arxiv.org/abs/1705.02315 http://dx.doi.org/10.1109/CVPR.2017.369},
year = {2017}
}
@article{Gordienko,
author = {Gordienko, Y and Kochura, Y and Alienin, O and arXiv preprint arXiv {\ldots}, O Rokovyi - and undefined 2018},
journal = {arxiv.org},
title = {{Dimensionality reduction in deep learning for chest x-ray analysis of lung cancer}},
url = {https://arxiv.org/abs/1801.06495}
}
@article{Zhou2018,
abstract = {We present a weakly supervised deep learning model for classifying thoracic diseases and identifying abnormalities in chest radiography. In this work, instead of learning from medical imaging data with region-level annotations, our model was merely trained on imaging data with image-level labels to classify diseases, and is able to identify abnormal image regions simultaneously. Our model consists of a customized pooling structure and an adaptive DenseNet front-end, which can effectively recognize possible disease features for classification and localization tasks. Our method has been validated on the publicly available ChestX-ray14 dataset. Experimental results have demonstrated that our classification and localization prediction performance achieved significant improvement over the previous models on the ChestX-ray14 dataset. In summary, our network can produce accurate disease classification and localization, which can potentially support clinical decisions.},
archivePrefix = {arXiv},
arxivId = {1807.01257},
author = {Zhou, Bo and Li, Yuemeng and Wang, Jiangcong},
eprint = {1807.01257},
month = {jul},
title = {{A Weakly Supervised Adaptive DenseNet for Classifying Thoracic Diseases and Identifying Abnormalities}},
url = {http://arxiv.org/abs/1807.01257},
year = {2018}
}
@article{Rajaraman2018,
abstract = {{\textless}p{\textgreater}Pneumonia affects 7{\%} of the global population, resulting in 2 million pediatric deaths every year. Chest X-ray (CXR) analysis is routinely performed to diagnose the disease. Computer-aided diagnostic (CADx) tools aim to supplement decision-making. These tools process the handcrafted and/or convolutional neural network (CNN) extracted image features for visual recognition. However, CNNs are perceived as black boxes since their performance lack explanations. This is a serious bottleneck in applications involving medical screening/diagnosis since poorly interpreted model behavior could adversely affect the clinical decision. In this study, we evaluate, visualize, and explain the performance of customized CNNs to detect pneumonia and further differentiate between bacterial and viral types in pediatric CXRs. We present a novel visualization strategy to localize the region of interest (ROI) that is considered relevant for model predictions across all the inputs that belong to an expected class. We statistically validate the models' performance toward the underlying tasks. We observe that the customized VGG16 model achieves 96.2{\%} and 93.6{\%} accuracy in detecting the disease and distinguishing between bacterial and viral pneumonia respectively. The model outperforms the state-of-the-art in all performance metrics and demonstrates reduced bias and improved generalization.{\textless}/p{\textgreater}},
author = {Rajaraman, Sivaramakrishnan and Candemir, Sema and Kim, Incheol and Thoma, George and Antani, Sameer and Rajaraman, Sivaramakrishnan and Candemir, Sema and Kim, Incheol and Thoma, George and Antani, Sameer},
doi = {10.3390/app8101715},
file = {:home/administrator/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rajaraman et al. - 2018 - Visualization and Interpretation of Convolutional Neural Network Predictions in Detecting Pneumonia in Pediatr.pdf:pdf},
issn = {2076-3417},
journal = {Applied Sciences},
keywords = {aided diagnosis,chest X,clinical decision,computer,computer vision,convolutional neural networks,explanation,pediatric,pneumonia,rays,visualization},
month = {sep},
number = {10},
pages = {1715},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Visualization and Interpretation of Convolutional Neural Network Predictions in Detecting Pneumonia in Pediatric Chest Radiographs}},
url = {http://www.mdpi.com/2076-3417/8/10/1715},
volume = {8},
year = {2018}
}
@incollection{Maicas2018,
author = {Maicas, Gabriel and Bradley, Andrew P. and Nascimento, Jacinto C. and Reid, Ian and Carneiro, Gustavo},
doi = {10.1007/978-3-030-00928-1_62},
month = {sep},
pages = {546--554},
publisher = {Springer, Cham},
title = {{Training Medical Image Analysis Systems like Radiologists}},
url = {http://link.springer.com/10.1007/978-3-030-00928-1{\_}62},
year = {2018}
}
@article{Khan2018,
abstract = {This thesis is submitted in partial fulfilment of the requirements for the degree of Bachelor of Science in Computer Science and Engineering, 2018.},
author = {Khan, Md. Rakib Hossain},
file = {:home/administrator/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Khan - 2018 - Deep learning based medical X-ray image recognition and classification.pdf:pdf},
keywords = {Image processing.,Pattern recognition systems.},
publisher = {BRAC University},
title = {{Deep learning based medical X-ray image recognition and classification}},
url = {http://dspace.bracu.ac.bd/xmlui/handle/10361/11426},
year = {2018}
}
@article{Rajpurkar2017,
abstract = {We develop an algorithm that can detect pneumonia from chest X-rays at a level exceeding practicing radiologists. Our algorithm, CheXNet, is a 121-layer convolutional neural network trained on ChestX-ray14, currently the largest publicly available chest X-ray dataset, containing over 100,000 frontal-view X-ray images with 14 diseases. Four practicing academic radiologists annotate a test set, on which we compare the performance of CheXNet to that of radiologists. We find that CheXNet exceeds average radiologist performance on the F1 metric. We extend CheXNet to detect all 14 diseases in ChestX-ray14 and achieve state of the art results on all 14 diseases.},
archivePrefix = {arXiv},
arxivId = {1711.05225},
author = {Rajpurkar, Pranav and Irvin, Jeremy and Zhu, Kaylie and Yang, Brandon and Mehta, Hershel and Duan, Tony and Ding, Daisy and Bagul, Aarti and Langlotz, Curtis and Shpanskaya, Katie and Lungren, Matthew P. and Ng, Andrew Y.},
doi = {1711.05225},
eprint = {1711.05225},
file = {:home/administrator/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rajpurkar et al. - 2017 - CheXNet Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning.pdf:pdf},
issn = {0015816X},
month = {nov},
title = {{CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning}},
url = {https://arxiv.org/abs/1711.05225 http://arxiv.org/abs/1711.05225},
year = {2017}
}
@article{Pesce2017,
abstract = {Machine learning approaches hold great potential for the automated detection of lung nodules in chest radiographs, but training the algorithms requires vary large amounts of manually annotated images, which are difficult to obtain. Weak labels indicating whether a radiograph is likely to contain pulmonary nodules are typically easier to obtain at scale by parsing historical free-text radiological reports associated to the radiographs. Using a repositotory of over 700,000 chest radiographs, in this study we demonstrate that promising nodule detection performance can be achieved using weak labels through convolutional neural networks for radiograph classification. We propose two network architectures for the classification of images likely to contain pulmonary nodules using both weak labels and manually-delineated bounding boxes, when these are available. Annotated nodules are used at training time to deliver a visual attention mechanism informing the model about its localisation performance. The first architecture extracts saliency maps from high-level convolutional layers and compares the estimated position of a nodule against the ground truth, when this is available. A corresponding localisation error is then back-propagated along with the softmax classification error. The second approach consists of a recurrent attention model that learns to observe a short sequence of smaller image portions through reinforcement learning. When a nodule annotation is available at training time, the reward function is modified accordingly so that exploring portions of the radiographs away from a nodule incurs a larger penalty. Our empirical results demonstrate the potential advantages of these architectures in comparison to competing methodologies.},
annote = {Attention},
archivePrefix = {arXiv},
arxivId = {1712.00996},
author = {Pesce, Emanuele and Ypsilantis, Petros-Pavlos and Withey, Samuel and Bakewell, Robert and Goh, Vicky and Montana, Giovanni},
doi = {10.1016/j.media.2018.12.007},
eprint = {1712.00996},
file = {:home/administrator/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pesce et al. - 2017 - Learning to detect chest radiographs containing lung nodules using visual attention networks.pdf:pdf},
month = {dec},
title = {{Learning to detect chest radiographs containing lung nodules using visual attention networks}},
url = {http://arxiv.org/abs/1712.00996 http://dx.doi.org/10.1016/j.media.2018.12.007},
year = {2017}
}
@article{VanNoord2017,
abstract = {Convolutional Neural Networks (CNNs) require large image corpora to be trained on classification tasks. The variation in image resolutions, sizes of objects and patterns depicted, and image scales, hampers CNN training and performance, because the task-relevant information varies over spatial scales. Previous work attempting to deal with such scale variations focused on encouraging scale-invariant CNN representations. However, scale-invariant representations are incomplete representations of images, because images contain scale-variant information as well. This paper addresses the combined development of scale-invariant and scale-variant representations. We propose a multi-scale CNN method to encourage the recognition of both types of features and evaluate it on a challenging image classification task involving task-relevant characteristics at multiple scales. The results show that our multi-scale CNN outperforms single-scale CNN. This leads to the conclusion that encouraging the combined development of a scale-invariant and scale-variant representation in CNNs is beneficial to image recognition performance.},
annote = {Similar to our method. Scale invariance?},
author = {van Noord, Nanne and Postma, Eric},
doi = {10.1016/J.PATCOG.2016.06.005},
file = {:home/administrator/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/van Noord, Postma - 2017 - Learning scale-variant and scale-invariant features for deep image classification.pdf:pdf},
issn = {0031-3203},
journal = {Pattern Recognition},
month = {jan},
pages = {583--592},
publisher = {Pergamon},
title = {{Learning scale-variant and scale-invariant features for deep image classification}},
url = {https://www.sciencedirect.com/science/article/pii/S0031320316301224},
volume = {61},
year = {2017}
}
@incollection{Wang2018,
author = {Wang, Qingfeng and Cheng, Jie-Zhi and Zhou, Ying and Zhuang, Hang and Li, Changlong and Chen, Bo and Liu, Zhiqin and Huang, Jun and Wang, Chao and Zhou, Xuehai},
doi = {10.1007/978-3-030-04239-4_38},
month = {dec},
pages = {420--432},
publisher = {Springer, Cham},
title = {{Low-Shot Multi-label Incremental Learning for Thoracic Diseases Diagnosis}},
url = {http://link.springer.com/10.1007/978-3-030-04239-4{\_}38},
year = {2018}
}
@article{Yao2018,
abstract = {Diagnostic imaging often requires the simultaneous identification of a multitude of findings of varied size and appearance. Beyond global indication of said findings, the prediction and display of localization information improves trust in and understanding of results when augmenting clinical workflow. Medical training data rarely includes more than global image-level labels as segmentations are time-consuming and expensive to collect. We introduce an approach to managing these practical constraints by applying a novel architecture which learns at multiple resolutions while generating saliency maps with weak supervision. Further, we parameterize the Log-Sum-Exp pooling function with a learnable lower-bounded adaptation (LSE-LBA) to build in a sharpness prior and better handle localizing abnormalities of different sizes using only image-level labels. Applying this approach to interpreting chest x-rays, we set the state of the art on 9 abnormalities in the NIH's CXR14 dataset while generating saliency maps with the highest resolution to date.},
annote = {From Duplicate 1 (Weakly Supervised Medical Diagnosis and Localization from Multiple Resolutions - Yao, Li; Prosky, Jordan; Poblenz, Eric; Covington, Ben; Lyman, Kevin)

Need to read.
Authors generate high-res heatmaps by learned convolutional upsampling},
archivePrefix = {arXiv},
arxivId = {1803.07703},
author = {Yao, Li and Prosky, Jordan and Poblenz, Eric and Covington, Ben and Lyman, Kevin},
eprint = {1803.07703},
file = {:home/administrator/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yao et al. - 2018 - Weakly Supervised Medical Diagnosis and Localization from Multiple Resolutions(3).pdf:pdf;:home/administrator/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yao et al. - 2018 - Weakly Supervised Medical Diagnosis and Localization from Multiple Resolutions(4).pdf:pdf},
month = {mar},
title = {{Weakly Supervised Medical Diagnosis and Localization from Multiple Resolutions}},
url = {http://arxiv.org/abs/1803.07703},
year = {2018}
}
@article{Xu2019,
author = {Xu, Shuaijing and Wu, Hao and Bie, Rongfang},
doi = {10.1109/ACCESS.2018.2885997},
issn = {2169-3536},
journal = {IEEE Access},
pages = {4466--4477},
title = {{CXNet-m1: Anomaly Detection on Chest X-Rays With Image-Based Deep Learning}},
url = {https://ieeexplore.ieee.org/document/8575127/},
volume = {7},
year = {2019}
}
@incollection{Shen2018,
author = {Shen, Yan and Gao, Mingchen},
doi = {10.1007/978-3-030-00919-9_45},
month = {sep},
pages = {389--397},
publisher = {Springer, Cham},
title = {{Dynamic Routing on Deep Neural Network for Thoracic Disease Classification and Sensitive Area Localization}},
url = {http://link.springer.com/10.1007/978-3-030-00919-9{\_}45},
year = {2018}
}
@incollection{Sedai2018,
annote = {Multiscale as in multiple resolutions?},
author = {Sedai, Suman and Mahapatra, Dwarikanath and Ge, Zongyuan and Chakravorty, Rajib and Garnavi, Rahil},
doi = {10.1007/978-3-030-00919-9_31},
month = {sep},
pages = {267--275},
publisher = {Springer, Cham},
title = {{Deep Multiscale Convolutional Feature Learning for Weakly Supervised Localization of Chest Pathologies in X-ray Images}},
url = {http://link.springer.com/10.1007/978-3-030-00919-9{\_}31},
year = {2018}
}
@incollection{Kumar2018,
author = {Kumar, Pulkit and Grewal, Monika and Srivastava, Muktabh Mayank},
doi = {10.1007/978-3-319-93000-8_62},
month = {jun},
pages = {546--552},
publisher = {Springer, Cham},
title = {{Boosted Cascaded Convnets for Multilabel Classification of Thoracic Diseases in Chest Radiographs}},
url = {http://link.springer.com/10.1007/978-3-319-93000-8{\_}62},
year = {2018}
}
@article{Yu2017,
abstract = {Multi-label image classification is a challenging problem in computer vision. Motivated by the recent development in image classification performance using Deep Neural Networks, in this work, we propose a flexible deep Convolutional Neural Network (CNN) framework, called Local-Global-CNN (LGC), to improve multi-label image classification performance. LGC consists of firstly a local level multi-label classifier which takes object segment hypotheses as inputs to a local CNN. The output results of these local hypotheses are aggregated together with max-pooling and then re-weighted to consider the label co-occurrence or interdependencies information by using a graphical model in the label space. LGC also utilizes a global CNN that is trained by multi-label images to directly predict the multiple labels from the input. The predictions of local and global level classifiers are finally fused together to obtain MAP estimation of the final multi-label prediction. The above LGC framework could benefit from a pre-train process with a large-scale single-label image dataset, e.g., ImageNet. Experimental results have shown that the proposed framework could achieve promising performance on Pascal VOC2007 and VOC2012 multi-label image dataset.},
annote = {Graphical model for learning interdependencies},
author = {Yu, Qinghua and Wang, Jinjun and Zhang, Shizhou and Gong, Yihong and Zhao, Jizhong},
doi = {10.1016/J.NEUCOM.2016.12.051},
issn = {0925-2312},
journal = {Neurocomputing},
month = {apr},
pages = {38--45},
publisher = {Elsevier},
title = {{Combining local and global hypotheses in deep neural network for multi-label image classification}},
url = {https://www.sciencedirect.com/science/article/pii/S092523121631579X},
volume = {235},
year = {2017}
}
@article{Yan,
author = {Yan, C and Yao, J and Li, R and Xu, Z and ACM, J Huang - Proceedings of the 2018 and undefined 2018},
file = {:home/administrator/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yan et al. - Unknown - Weakly Supervised Deep Learning for Thoracic Disease Classification and Localization on Chest X-rays.pdf:pdf},
journal = {dl.acm.org},
title = {{Weakly Supervised Deep Learning for Thoracic Disease Classification and Localization on Chest X-rays}},
url = {https://dl.acm.org/citation.cfm?id=3233573}
}
@article{Yao,
author = {Yao, L and Poblenz, E and Dagunts, D and arXiv preprint arXiv {\ldots}, B Covington - and undefined 2017},
journal = {arxiv.org},
title = {{Learning to diagnose from scratch by exploiting dependencies among labels}},
url = {https://arxiv.org/abs/1710.10501}
}
@article{Rajpurkar2018,
author = {Rajpurkar, Pranav and Irvin, Jeremy and Ball, Robyn L. and Zhu, Kaylie and Yang, Brandon and Mehta, Hershel and Duan, Tony and Ding, Daisy and Bagul, Aarti and Langlotz, Curtis P. and Patel, Bhavik N. and Yeom, Kristen W. and Shpanskaya, Katie and Blankenberg, Francis G. and Seekins, Jayne and Amrhein, Timothy J. and Mong, David A. and Halabi, Safwan S. and Zucker, Evan J. and Ng, Andrew Y. and Lungren, Matthew P.},
doi = {10.1371/journal.pmed.1002686},
editor = {Sheikh, Aziz},
issn = {1549-1676},
journal = {PLOS Medicine},
month = {nov},
number = {11},
pages = {e1002686},
title = {{Deep learning for chest radiograph diagnosis: A retrospective comparison of the CheXNeXt algorithm to practicing radiologists}},
url = {http://dx.plos.org/10.1371/journal.pmed.1002686},
volume = {15},
year = {2018}
}
@article{Bustos2019,
abstract = {We present a labeled large-scale, high resolution chest x-ray dataset for the automated exploration of medical images along with their associated reports. This dataset includes more than 160,000 images obtained from 67,000 patients that were interpreted and reported by radiologists at Hospital San Juan Hospital (Spain) from 2009 to 2017, covering six different position views and additional information on image acquisition and patient demography. The reports were labeled with 174 different radiographic findings, 19 differential diagnoses and 104 anatomic locations organized as a hierarchical taxonomy and mapped onto standard Unified Medical Language System (UMLS) terminology. Of these reports, 27{\%} were manually annotated by trained physicians and the remaining set was labeled using a supervised method based on a recurrent neural network with attention mechanisms. The labels generated were then validated in an independent test set achieving a 0.93 Micro-F1 score. To the best of our knowledge, this is one of the largest public chest x-ray database suitable for training supervised models concerning radiographs, and the first to contain radiographic reports in Spanish. The PadChest dataset can be downloaded from http://bimcv.cipf.es/bimcv-projects/padchest/.},
archivePrefix = {arXiv},
arxivId = {1901.07441},
author = {Bustos, Aurelia and Pertusa, Antonio and Salinas, Jose-Maria and de la Iglesia-Vay{\'{a}}, Maria},
eprint = {1901.07441},
file = {:home/administrator/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bustos et al. - 2019 - PadChest A large chest x-ray image dataset with multi-label annotated reports.pdf:pdf},
month = {jan},
title = {{PadChest: A large chest x-ray image dataset with multi-label annotated reports}},
url = {http://arxiv.org/abs/1901.07441},
year = {2019}
}
@techreport{Wasserman,
author = {Wasserman, Larry and Berlin, Springer and New, Heidelberg and Barcelona, York and Kong, Hong and Milan, London and Tokyo, Paris},
file = {:home/administrator/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wasserman et al. - Unknown - A Concise Course in Nonparametric Statistical Inference.pdf:pdf},
title = {{A Concise Course in Nonparametric Statistical Inference}},
url = {http://www.stat.cmu.edu/{~}larry/all-of-nonpar/contents.pdf}
}
@techreport{Antin,
author = {Antin, Benjamin and Kravitz, Joshua and Martayan, Emil},
file = {:home/administrator/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Antin, Kravitz, Martayan - Unknown - Detecting Pneumonia in Chest X-Rays with Supervised Learning.pdf:pdf},
title = {{Detecting Pneumonia in Chest X-Rays with Supervised Learning}},
url = {https://pdfs.semanticscholar.org/bbc7/49a5c9139dc642a78647c1dfed1df71bba07.pdf}
}
@incollection{Dai2018,
author = {Dai, Wei and Dong, Nanqing and Wang, Zeya and Liang, Xiaodan and Zhang, Hao and Xing, Eric P.},
doi = {10.1007/978-3-030-00889-5_30},
pages = {263--273},
title = {{SCAN: Structure Correcting Adversarial Network for Organ Segmentation in Chest X-Rays}},
url = {http://link.springer.com/10.1007/978-3-030-00889-5{\_}30},
year = {2018}
}
@article{Guan2018,
abstract = {This paper considers the task of thorax disease classification on chest X-ray images. Existing methods generally use the global image as input for network learning. Such a strategy is limited in two aspects. 1) A thorax disease usually happens in (small) localized areas which are disease specific. Training CNNs using global image may be affected by the (excessive) irrelevant noisy areas. 2) Due to the poor alignment of some CXR images, the existence of irregular borders hinders the network performance. In this paper, we address the above problems by proposing a three-branch attention guided convolution neural network (AG-CNN). AG-CNN 1) learns from disease-specific regions to avoid noise and improve alignment, 2) also integrates a global branch to compensate the lost discriminative cues by local branch. Specifically, we first learn a global CNN branch using global images. Then, guided by the attention heat map generated from the global branch, we inference a mask to crop a discriminative region from the global image. The local region is used for training a local CNN branch. Lastly, we concatenate the last pooling layers of both the global and local branches for fine-tuning the fusion branch. The Comprehensive experiment is conducted on the ChestX-ray14 dataset. We first report a strong global baseline producing an average AUC of 0.841 with ResNet-50 as backbone. After combining the local cues with the global information, AG-CNN improves the average AUC to 0.868. While DenseNet-121 is used, the average AUC achieves 0.871, which is a new state of the art in the community.},
annote = {Attention},
archivePrefix = {arXiv},
arxivId = {1801.09927},
author = {Guan, Qingji and Huang, Yaping and Zhong, Zhun and Zheng, Zhedong and Zheng, Liang and Yang, Yi},
eprint = {1801.09927},
month = {jan},
title = {{Diagnose like a Radiologist: Attention Guided Convolutional Neural Network for Thorax Disease Classification}},
url = {http://arxiv.org/abs/1801.09927},
year = {2018}
}
@article{Stirenko,
author = {Stirenko, S and Kochura, Y and {\ldots}, O Alienin - 2018 IEEE 38th and undefined 2018},
journal = {ieeexplore.ieee.org},
title = {{Chest X-Ray Analysis of Tuberculosis by Deep Learning with Segmentation and Augmentation}},
url = {https://ieeexplore.ieee.org/abstract/document/8477564/}
}
@misc{Hwang2018,
author = {Hwang, Eui Jin and Park, Sunggyun and Jin, Kwang-Nam and Kim, Jung Im and Choi, So Young and Lee, Jong Hyuk and Goo, Jin Mo and Aum, Jaehong and Yim, Jae-Joon and Cohen, Julien G. and Ferretti, Gilbert R. and Park, Chang Min},
keywords = {Chest Radiograph,Deep Learning,Lung Cancer,Pneumonia,Pneumothorax,Tuberculosis},
month = {nov},
title = {{Development and Validation of a Deep Learning-Based Automated Detection Algorithm for Major Thoracic Diseases on Chest Radiographs}},
url = {https://papers.ssrn.com/sol3/papers.cfm?abstract{\_}id=3289800},
year = {2018}
}
@article{Zech2018,
abstract = {Background There is interest in using convolutional neural networks (CNNs) to analyze medical imaging to provide computer-aided diagnosis (CAD). Recent work has suggested that image classification CNNs may not generalize to new data as well as previously believed. We assessed how well CNNs generalized across three hospital systems for a simulated pneumonia screening task.   Methods and findings A cross-sectional design with multiple model training cohorts was used to evaluate model generalizability to external sites using split-sample validation. A total of 158,323 chest radiographs were drawn from three institutions: National Institutes of Health Clinical Center (NIH; 112,120 from 30,805 patients), Mount Sinai Hospital (MSH; 42,396 from 12,904 patients), and Indiana University Network for Patient Care (IU; 3,807 from 3,683 patients). These patient populations had an age mean (SD) of 46.9 years (16.6), 63.2 years (16.5), and 49.6 years (17) with a female percentage of 43.5{\%}, 44.8{\%}, and 57.3{\%}, respectively. We assessed individual models using the area under the receiver operating characteristic curve (AUC) for radiographic findings consistent with pneumonia and compared performance on different test sets with DeLong's test. The prevalence of pneumonia was high enough at MSH (34.2{\%}) relative to NIH and IU (1.2{\%} and 1.0{\%}) that merely sorting by hospital system achieved an AUC of 0.861 (95{\%} CI 0.855–0.866) on the joint MSH–NIH dataset. Models trained on data from either NIH or MSH had equivalent performance on IU (P values 0.580 and 0.273, respectively) and inferior performance on data from each other relative to an internal test set (i.e., new data from within the hospital system used for training data; P values both {\textless}0.001). The highest internal performance was achieved by combining training and test data from MSH and NIH (AUC 0.931, 95{\%} CI 0.927–0.936), but this model demonstrated significantly lower external performance at IU (AUC 0.815, 95{\%} CI 0.745–0.885, P = 0.001). To test the effect of pooling data from sites with disparate pneumonia prevalence, we used stratified subsampling to generate MSH–NIH cohorts that only differed in disease prevalence between training data sites. When both training data sites had the same pneumonia prevalence, the model performed consistently on external IU data (P = 0.88). When a 10-fold difference in pneumonia rate was introduced between sites, internal test performance improved compared to the balanced model (10× MSH risk P {\textless} 0.001; 10× NIH P = 0.002), but this outperformance failed to generalize to IU (MSH 10× P {\textless} 0.001; NIH 10× P = 0.027). CNNs were able to directly detect hospital system of a radiograph for 99.95{\%} NIH (22,050/22,062) and 99.98{\%} MSH (8,386/8,388) radiographs. The primary limitation of our approach and the available public data is that we cannot fully assess what other factors might be contributing to hospital system–specific biases.   Conclusion Pneumonia-screening CNNs achieved better internal than external performance in 3 out of 5 natural comparisons. When models were trained on pooled data from sites with different pneumonia prevalence, they performed better on new pooled data from these sites but not on external data. CNNs robustly identified hospital system and department within a hospital, which can have large differences in disease burden and may confound predictions.},
author = {Zech, John R. and Badgeley, Marcus A. and Liu, Manway and Costa, Anthony B. and Titano, Joseph J. and Oermann, Eric Karl},
doi = {10.1371/journal.pmed.1002683},
editor = {Sheikh, Aziz},
file = {:home/administrator/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zech et al. - 2018 - Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs A cross-secti.pdf:pdf},
issn = {1549-1676},
journal = {PLOS Medicine},
month = {nov},
number = {11},
pages = {e1002683},
publisher = {Public Library of Science},
title = {{Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study}},
url = {http://dx.plos.org/10.1371/journal.pmed.1002683},
volume = {15},
year = {2018}
}
@incollection{AsgariTaghanaki2018,
author = {{Asgari Taghanaki}, Saeid and Das, Arkadeep and Hamarneh, Ghassan},
doi = {10.1007/978-3-030-02628-8_10},
month = {sep},
pages = {87--94},
publisher = {Springer, Cham},
title = {{Vulnerability Analysis of Chest X-Ray Image Classification Against Adversarial Attacks}},
url = {http://link.springer.com/10.1007/978-3-030-02628-8{\_}10},
year = {2018}
}
@article{Shih2019,
author = {Shih, George and Wu, Carol C. and Halabi, Safwan S. and Kohli, Marc D. and Prevedello, Luciano M. and Cook, Tessa S. and Sharma, Arjun and Amorosa, Judith K. and Arteaga, Veronica and Galperin-Aizenberg, Maya and Gill, Ritu R. and Godoy, Myrna C.B. and Hobbs, Stephen and Jeudy, Jean and Laroia, Archana and Shah, Palmi N. and Vummidi, Dharshan and Yaddanapudi, Kavitha and Stein, Anouk},
doi = {10.1148/ryai.2019180041},
issn = {2638-6100},
journal = {Radiology: Artificial Intelligence},
month = {jan},
number = {1},
pages = {e180041},
publisher = { Radiological Society of North America },
title = {{Augmenting the National Institutes of Health Chest Radiograph Dataset with Expert Annotations of Possible Pneumonia}},
url = {http://pubs.rsna.org/doi/10.1148/ryai.2019180041},
volume = {1},
year = {2019}
}
@article{Gordienkoa,
author = {Gordienko, Y and Gang, P and Hui, J and Zeng, W and {\ldots}, Y Kochura - {\ldots} Conference on Theory and undefined 2018},
file = {:home/administrator/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gordienko et al. - Unknown - Deep learning with lung segmentation and bone shadow exclusion techniques for chest x-ray analysis of lung.pdf:pdf},
journal = {Springer},
title = {{Deep learning with lung segmentation and bone shadow exclusion techniques for chest x-ray analysis of lung cancer}},
url = {https://link.springer.com/chapter/10.1007/978-3-319-91008-6{\_}63}
}
@article{Irvin2019,
abstract = {Large, labeled datasets have driven deep learning methods to achieve expert-level performance on a variety of medical imaging tasks. We present CheXpert, a large dataset that contains 224,316 chest radiographs of 65,240 patients. We design a labeler to automatically detect the presence of 14 observations in radiology reports, capturing uncertainties inherent in radiograph interpretation. We investigate different approaches to using the uncertainty labels for training convolutional neural networks that output the probability of these observations given the available frontal and lateral radiographs. On a validation set of 200 chest radiographic studies which were manually annotated by 3 board-certified radiologists, we find that different uncertainty approaches are useful for different pathologies. We then evaluate our best model on a test set composed of 500 chest radiographic studies annotated by a consensus of 5 board-certified radiologists, and compare the performance of our model to that of 3 additional radiologists in the detection of 5 selected pathologies. On Cardiomegaly, Edema, and Pleural Effusion, the model ROC and PR curves lie above all 3 radiologist operating points. We release the dataset to the public as a standard benchmark to evaluate performance of chest radiograph interpretation models. The dataset is freely available at https://stanfordmlgroup.github.io/competitions/chexpert .},
archivePrefix = {arXiv},
arxivId = {1901.07031},
author = {Irvin, Jeremy and Rajpurkar, Pranav and Ko, Michael and Yu, Yifan and Ciurea-Ilcus, Silviana and Chute, Chris and Marklund, Henrik and Haghgoo, Behzad and Ball, Robyn and Shpanskaya, Katie and Seekins, Jayne and Mong, David A. and Halabi, Safwan S. and Sandberg, Jesse K. and Jones, Ricky and Larson, David B. and Langlotz, Curtis P. and Patel, Bhavik N. and Lungren, Matthew P. and Ng, Andrew Y.},
eprint = {1901.07031},
file = {:home/administrator/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Irvin et al. - 2019 - CheXpert A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison.pdf:pdf},
month = {jan},
title = {{CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison}},
url = {http://arxiv.org/abs/1901.07031},
year = {2019}
}
@article{Xu2018,
author = {Xu, Xiuyuan and Guo, Quan and Guo, Jixiang and Yi, Zhang},
doi = {10.1109/ACCESS.2018.2875406},
issn = {2169-3536},
journal = {IEEE Access},
pages = {66972--66983},
title = {{DeepCXray: Automatically Diagnosing Diseases on Chest X-Rays Using Deep Neural Networks}},
url = {https://ieeexplore.ieee.org/document/8489927/},
volume = {6},
year = {2018}
}
@article{Abiyev2018,
abstract = {Chest diseases are very serious health problems in the life of people. These diseases include chronic obstructive pulmonary disease, pneumonia, asthma, tuberculosis, and lung diseases. The timely diagnosis of chest diseases is very important. Many methods have been developed for this purpose. In this paper, we demonstrate the feasibility of classifying the chest pathologies in chest X-rays using conventional and deep learning approaches. In the paper, convolutional neural networks (CNNs) are presented for the diagnosis of chest diseases. The architecture of CNN and its design principle are presented. For comparative purpose, backpropagation neural networks (BPNNs) with supervised learning, competitive neural networks (CpNNs) with unsupervised learning are also constructed for diagnosis chest diseases. All the considered networks CNN, BPNN, and CpNN are trained and tested on the same chest X-ray database, and the performance of each network is discussed. Comparative results in terms of accuracy, error rate, and training time between the networks are presented.},
author = {Abiyev, Rahib H. and Ma'aitah, Mohammad Khaleel Sallam},
doi = {10.1155/2018/4168538},
issn = {2040-2295},
journal = {Journal of Healthcare Engineering},
month = {aug},
pages = {1--11},
publisher = {Hindawi},
title = {{Deep Convolutional Neural Networks for Chest Diseases Detection}},
url = {https://www.hindawi.com/journals/jhe/2018/4168538/},
volume = {2018},
year = {2018}
}
@article{Lakhani2017,
abstract = {Deep learning with convolutional neural networks can accurately classify tuberculosis at chest radiography with an area under the curve of 0.99.},
author = {Lakhani, Paras and Sundaram, Baskaran},
doi = {10.1148/radiol.2017162326},
issn = {0033-8419},
journal = {Radiology},
month = {aug},
number = {2},
pages = {574--582},
publisher = {Radiological Society of North America},
title = {{Deep Learning at Chest Radiography: Automated Classification of Pulmonary Tuberculosis by Using Convolutional Neural Networks}},
url = {http://pubs.rsna.org/doi/10.1148/radiol.2017162326},
volume = {284},
year = {2017}
}
@inproceedings{Yan2018,
address = {New York, New York, USA},
author = {Yan, Chaochao and Yao, Jiawen and Li, Ruoyu and Xu, Zheng and Huang, Junzhou},
booktitle = {Proceedings of the 2018 ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics  - BCB '18},
doi = {10.1145/3233547.3233573},
isbn = {9781450357944},
keywords = {chest x-ray,computer-aided diagnosis,weakly-supervised learning},
pages = {103--110},
publisher = {ACM Press},
title = {{Weakly Supervised Deep Learning for Thoracic Disease Classification and Localization on Chest X-rays}},
url = {http://dl.acm.org/citation.cfm?doid=3233547.3233573},
year = {2018}
}
@article{Haloi2018,
abstract = {In this work, we propose advanced pneumonia and Tuberculosis grading system for X-ray images. The proposed system is a very deep fully convolutional classification network with online augmentation that outputs confidence values for diseases prevalence. Its a fully automated system capable of disease feature understanding without any offline preprocessing step or manual feature extraction. We have achieved state- of-the- art performance on the public databases such as ChestXray-14, Mendeley, Shenzhen Hospital X-ray and Belarus X-ray set.},
archivePrefix = {arXiv},
arxivId = {1807.03120},
author = {Haloi, Mrinal and Rajalakshmi, K. Raja and Walia, Pradeep},
eprint = {1807.03120},
file = {:home/administrator/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Haloi, Rajalakshmi, Walia - 2018 - Towards Radiologist-Level Accurate Deep Learning System for Pulmonary Screening(2).pdf:pdf},
month = {jun},
title = {{Towards Radiologist-Level Accurate Deep Learning System for Pulmonary Screening}},
url = {http://arxiv.org/abs/1807.03120},
year = {2018}
}
@incollection{Gordienko2019,
author = {Gordienko, Yu. and Gang, Peng and Hui, Jiang and Zeng, Wei and Kochura, Yu. and Alienin, O. and Rokovyi, O. and Stirenko, S.},
doi = {10.1007/978-3-319-91008-6_63},
pages = {638--647},
title = {{Deep Learning with Lung Segmentation and Bone Shadow Exclusion Techniques for Chest X-Ray Analysis of Lung Cancer}},
url = {http://link.springer.com/10.1007/978-3-319-91008-6{\_}63},
year = {2019}
}
@article{Wang2018a,
abstract = {Despite the state-of-the-art performance for medical image segmentation, deep convolutional neural networks (CNNs) have rarely provided uncertainty estimations regarding their segmentation outputs, e.g., model (epistemic) and image-based (aleatoric) uncertainties. In this work, we analyze these different types of uncertainties for CNN-based 2D and 3D medical image segmentation tasks. We additionally propose a test-time augmentation-based aleatoric uncertainty to analyze the effect of different transformations of the input image on the segmentation output. Test-time augmentation has been previously used to improve segmentation accuracy, yet not been formulated in a consistent mathematical framework. Hence, we also propose a theoretical formulation of test-time augmentation, where a distribution of the prediction is estimated by Monte Carlo simulation with prior distributions of parameters in an image acquisition model that involves image transformations and noise. We compare and combine our proposed aleatoric uncertainty with model uncertainty. Experiments with segmentation of fetal brains and brain tumors from 2D and 3D Magnetic Resonance Images (MRI) showed that 1) the test-time augmentation-based aleatoric uncertainty provides a better uncertainty estimation than calculating the test-time dropout-based model uncertainty alone and helps to reduce overconfident incorrect predictions, and 2) our test-time augmentation outperforms a single-prediction baseline and dropout-based multiple predictions.},
archivePrefix = {arXiv},
arxivId = {1807.07356},
author = {Wang, Guotai and Li, Wenqi and Aertsen, Michael and Deprest, Jan and Ourselin, Sebastien and Vercauteren, Tom},
doi = {10.1016/j.neucom.2019.01.103},
eprint = {1807.07356},
file = {:home/administrator/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2018 - Aleatoric uncertainty estimation with test-time augmentation for medical image segmentation with convolutional neur.pdf:pdf},
month = {jul},
title = {{Aleatoric uncertainty estimation with test-time augmentation for medical image segmentation with convolutional neural networks}},
url = {http://arxiv.org/abs/1807.07356 http://dx.doi.org/10.1016/j.neucom.2019.01.103},
year = {2018}
}
@incollection{Cai2018,
annote = {Attention},
author = {Cai, Jinzheng and Lu, Le and Harrison, Adam P. and Shi, Xiaoshuang and Chen, Pingjun and Yang, Lin},
doi = {10.1007/978-3-030-00934-2_66},
month = {sep},
pages = {589--598},
publisher = {Springer, Cham},
title = {{Iterative Attention Mining for Weakly Supervised Thoracic Disease Pattern Localization in Chest X-Rays}},
url = {http://link.springer.com/10.1007/978-3-030-00934-2{\_}66},
year = {2018}
}
@inproceedings{Sivaramakrishnan2017,
author = {Sivaramakrishnan, R. and Antani, S. and Xue, Z. and Candemir, S. and Jaeger, S. and Thoma, G. R.},
booktitle = {2017 IEEE Life Sciences Conference (LSC)},
doi = {10.1109/LSC.2017.8268146},
isbn = {978-1-5386-1030-5},
month = {dec},
pages = {71--74},
publisher = {IEEE},
title = {{Visualizing abnormalities in chest radiographs through salient network activations in Deep Learning}},
url = {http://ieeexplore.ieee.org/document/8268146/},
year = {2017}
}
@article{Guan2018a,
abstract = {This paper considers the problem of multi-label thorax disease classification on chest X-ray images. Identifying one or more pathologies from a chest X-ray image is often hindered by the pathologies unrelated to the targets. In this paper, we address the above problem by proposing a category-wise residual attention learning (CRAL) framework. CRAL predicts the presence of multiple pathologies in a class-specific attentive view. It aims to suppress the obstacles of irrelevant classes by endowing small weights to the corresponding feature representation. Meanwhile, the relevant features would be strengthened by assigning larger weights. Specifically, the proposed framework consists of two modules: feature embedding module and attention learning module. The feature embedding module learns high-level features with a convolutional neural network (CNN) while the attention learning module focuses on exploring the assignment scheme of different categories. The attention module can be flexibly integrated into any feature embedding networks with end-to-end training. The comprehensive experiments are conducted on the Chest X-ray14 dataset. CRAL yields the average AUC score of 0.816 which is a new state of the art.},
annote = {Another form of attention},
author = {Guan, Qingji and Huang, Yaping},
doi = {10.1016/J.PATREC.2018.10.027},
issn = {0167-8655},
journal = {Pattern Recognition Letters},
month = {oct},
publisher = {North-Holland},
title = {{Multi-label chest X-ray image classification via category-wise residual attention learning}},
url = {https://www.sciencedirect.com/science/article/abs/pii/S0167865518308559},
year = {2018}
}
@article{Rajpurkar,
author = {Rajpurkar, P and Irvin, J and Ball, RL and Zhu, K and {\ldots}, B Yang - PLoS and undefined 2018},
journal = {journals.plos.org},
title = {{Deep learning for chest radiograph diagnosis: A retrospective comparison of the CheXNeXt algorithm to practicing radiologists}},
url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1002686}
}
@article{VonBerg2016,
author = {von Berg, Jens and Young, Stewart and Carolus, Heike and Wolz, Robin and Saalbach, Axel and Hidalgo, Alberto and Gim{\'{e}}nez, Ana and Franquet, Tom{\'{a}}s},
doi = {10.1007/s11548-015-1278-y},
file = {:home/administrator/swaroop/papers/bone suppression.pdf:pdf},
issn = {1861-6410},
journal = {International Journal of Computer Assisted Radiology and Surgery},
month = {apr},
number = {4},
pages = {641--655},
publisher = {Springer Berlin Heidelberg},
title = {{A novel bone suppression method that improves lung nodule detection}},
url = {http://link.springer.com/10.1007/s11548-015-1278-y},
volume = {11},
year = {2016}
}
@incollection{Tang2018,
annote = {Attention and curriculum learning},
author = {Tang, Yuxing and Wang, Xiaosong and Harrison, Adam P. and Lu, Le and Xiao, Jing and Summers, Ronald M.},
doi = {10.1007/978-3-030-00919-9_29},
month = {sep},
pages = {249--258},
publisher = {Springer, Cham},
title = {{Attention-Guided Curriculum Learning for Weakly Supervised Classification and Localization of Thoracic Diseases on Chest Radiographs}},
url = {http://link.springer.com/10.1007/978-3-030-00919-9{\_}29},
year = {2018}
}
@article{Li,
author = {Li, Z and Wang, C and Han, M and Xue, Y and of the {\ldots}, W Wei - Proceedings and undefined 2018},
journal = {openaccess.thecvf.com},
title = {{Thoracic disease identification and localization with limited supervision}},
url = {http://openaccess.thecvf.com/content{\_}cvpr{\_}2018/html/Li{\_}Thoracic{\_}Disease{\_}Identification{\_}CVPR{\_}2018{\_}paper.html}
}
@article{Baltruschat2018,
abstract = {Chest radiography is the most common clinical examination type. To improve the quality of patient care and to reduce workload, methods for automatic pathology classification have been developed. In this contribution we investigate the usefulness of two advanced image pre-processing techniques, initially developed for image reading by radiologists, for the performance of Deep Learning methods. First, we use bone suppression, an algorithm to artificially remove the rib cage. Secondly, we employ an automatic lung field detection to crop the image to the lung area. Furthermore, we consider the combination of both in the context of an ensemble approach. In a five-times re-sampling scheme, we use Receiver Operating Characteristic (ROC) statistics to evaluate the effect of the pre-processing approaches. Using a Convolutional Neural Network (CNN), optimized for X-ray analysis, we achieve a good performance with respect to all pathologies on average. Superior results are obtained for selected pathologies when using pre-processing, i.e. for mass the area under the ROC curve increased by 9.95{\%}. The ensemble with pre-processed trained models yields the best overall results.},
annote = {Bone suppression and segmentation},
archivePrefix = {arXiv},
arxivId = {1810.07500},
author = {Baltruschat, Ivo M. and Steinmeister, Leonhard and Ittrich, Harald and Adam, Gerhard and Nickisch, Hannes and Saalbach, Axel and von Berg, Jens and Grass, Michael and Knopp, Tobias},
eprint = {1810.07500},
file = {:home/administrator/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baltruschat et al. - 2018 - When does Bone Suppression and Lung Field Segmentation Improve Chest X-Ray Disease Classification.pdf:pdf},
month = {oct},
title = {{When does Bone Suppression and Lung Field Segmentation Improve Chest X-Ray Disease Classification?}},
url = {http://arxiv.org/abs/1810.07500},
year = {2018}
}
@techreport{Johnson,
abstract = {Chest radiography is an extremely powerful imaging modality, allowing for a detailed inspection of a patient's thorax, but requiring specialized training for proper interpretation. With the advent of high performance general purpose computer vision algorithms, the accurate automated analysis of chest radiographs is becoming increasingly of interest to researchers. However, a key challenge in the development of these techniques is the lack of sufficient data. Here we describe MIMIC-CXR, a large dataset of 371,920 chest x-rays associated with 227,943 imaging studies sourced from the Beth Israel Deaconess Medical Center between 2011-2016. Each imaging study can pertain to one or more images, but most often are associated with two images: a frontal view and a lateral view. Images are provided with 14 labels derived from a natural language processing tool applied to the corresponding free-text radiology reports. All images have been de-identified to protect patient privacy. The dataset is made freely available to facilitate and encourage a wide range of research in medical computer vision.},
archivePrefix = {arXiv},
arxivId = {1901.07042v2},
author = {Johnson, Alistair E W and Pollard, Tom J and Berkowitz, Seth J and Greenbaum, Nathaniel R and Lungren, Matthew P and Deng, Chih-Ying and Mark, Roger G and Horng, Steven},
eprint = {1901.07042v2},
file = {:home/administrator/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Johnson et al. - Unknown - MIMIC-CXR A LARGE PUBLICLY AVAILABLE DATABASE OF LABELED CHEST RADIOGRAPHS.pdf:pdf},
keywords = {healthcare {\textperiodcentered} radiology {\textperiodcentered} computer vision},
title = {{MIMIC-CXR: A LARGE PUBLICLY AVAILABLE DATABASE OF LABELED CHEST RADIOGRAPHS}},
url = {https://github.com/ncbi-nlp/NegBio}
}
@article{Rajpurkar2018a,
abstract = {Background Chest radiograph interpretation is critical for the detection of thoracic diseases, including tuberculosis and lung cancer, which affect millions of people worldwide each year. This time-consuming task typically requires expert radiologists to read the images, leading to fatigue-based diagnostic error and lack of diagnostic expertise in areas of the world where radiologists are not available. Recently, deep learning approaches have been able to achieve expert-level performance in medical image interpretation tasks, powered by large network architectures and fueled by the emergence of large labeled datasets. The purpose of this study is to investigate the performance of a deep learning algorithm on the detection of pathologies in chest radiographs compared with practicing radiologists. Methods and findings We developed CheXNeXt, a convolutional neural network to concurrently detect the presence of 14 different pathologies, including pneumonia, pleural effusion, pulmonary masses, and nodules in frontal-view chest radiographs. CheXNeXt was trained and internally validated on the ChestX-ray8 dataset, with a held-out validation set consisting of 420 images, sampled to contain at least 50 cases of each of the original pathology labels. On this validation set, the majority vote of a panel of 3 board-certified cardiothoracic specialist radiologists served as reference standard. We compared CheXNeXt's discriminative performance on the validation set to the performance of 9 radiologists using the area under the receiver operating characteristic curve (AUC). The radiologists included 6 board-certified radiologists (average experience 12 years, range 4–28 years) and 3 senior radiology residents, from 3 academic institutions. We found that CheXNeXt achieved radiologist-level performance on 11 pathologies and did not achieve radiologist-level performance on 3 pathologies. The radiologists achieved statistically significantly higher AUC performance on cardiomegaly, emphysema, and hiatal hernia, with AUCs of 0.888 (95{\%} confidence interval [CI] 0.863–0.910), 0.911 (95{\%} CI 0.866–0.947), and 0.985 (95{\%} CI 0.974–0.991), respectively, whereas CheXNeXt's AUCs were 0.831 (95{\%} CI 0.790–0.870), 0.704 (95{\%} CI 0.567–0.833), and 0.851 (95{\%} CI 0.785–0.909), respectively. CheXNeXt performed better than radiologists in detecting atelectasis, with an AUC of 0.862 (95{\%} CI 0.825–0.895), statistically significantly higher than radiologists' AUC of 0.808 (95{\%} CI 0.777–0.838); there were no statistically significant differences in AUCs for the other 10 pathologies. The average time to interpret the 420 images in the validation set was substantially longer for the radiologists (240 minutes) than for CheXNeXt (1.5 minutes). The main limitations of our study are that neither CheXNeXt nor the radiologists were permitted to use patient history or review prior examinations and that evaluation was limited to a dataset from a single institution. Conclusions In this study, we developed and validated a deep learning algorithm that classified clinically important abnormalities in chest radiographs at a performance level comparable to practicing radiologists. Once tested prospectively in clinical settings, the algorithm could have the potential to expand patient access to chest radiograph diagnostics.},
author = {Rajpurkar, Pranav and Irvin, Jeremy and Ball, Robyn L. and Zhu, Kaylie and Yang, Brandon and Mehta, Hershel and Duan, Tony and Ding, Daisy and Bagul, Aarti and Langlotz, Curtis P. and Patel, Bhavik N. and Yeom, Kristen W. and Shpanskaya, Katie and Blankenberg, Francis G. and Seekins, Jayne and Amrhein, Timothy J. and Mong, David A. and Halabi, Safwan S. and Zucker, Evan J. and Ng, Andrew Y. and Lungren, Matthew P.},
doi = {10.1371/journal.pmed.1002686},
editor = {Sheikh, Aziz},
file = {:home/administrator/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rajpurkar et al. - 2018 - Deep learning for chest radiograph diagnosis A retrospective comparison of the CheXNeXt algorithm to practicin.pdf:pdf},
isbn = {1111111111},
issn = {15491676},
journal = {PLoS Medicine},
month = {nov},
number = {11},
pages = {e1002686},
pmid = {30457988},
publisher = {Public Library of Science},
title = {{Deep learning for chest radiograph diagnosis: A retrospective comparison of the CheXNeXt algorithm to practicing radiologists}},
url = {http://dx.plos.org/10.1371/journal.pmed.1002686},
volume = {15},
year = {2018}
}
@article{Islam2017,
abstract = {Chest X-Rays (CXRs) are widely used for diagnosing abnormalities in the heart and lung area. Automatically detecting these abnormalities with high accuracy could greatly enhance real world diagnosis processes. Lack of standard publicly available dataset and benchmark studies, however, makes it difficult to compare various detection methods. In order to overcome these difficulties, we have used a publicly available Indiana CXR, JSRT and Shenzhen dataset and studied the performance of known deep convolutional network (DCN) architectures on different abnormalities. We find that the same DCN architecture doesn't perform well across all abnormalities. Shallow features or earlier layers consistently provide higher detection accuracy compared to deep features. We have also found ensemble models to improve classification significantly compared to single model. Combining these insight, we report the highest accuracy on chest X-Ray abnormality detection on these datasets. We find that for cardiomegaly detection, the deep learning method improves the accuracy by a staggering 17 percentage point compared to rule based methods. We applied the techniques to the problem of tuberculosis detection on a different dataset and achieved the highest accuracy. Our localization experiments using these trained classifiers show that for spatially spread out abnormalities like cardiomegaly and pulmonary edema, the network can localize the abnormalities successfully most of the time. One remarkable result of the cardiomegaly localization is that the heart and its surrounding region is most responsible for cardiomegaly detection, in contrast to the rule based models where the ratio of heart and lung area is used as the measure. We believe that through deep learning based classification and localization, we will discover many more interesting features in medical image diagnosis that are not considered traditionally.},
archivePrefix = {arXiv},
arxivId = {1705.09850},
author = {Islam, Mohammad Tariqul and Aowal, Md Abdul and Minhaz, Ahmed Tahseen and Ashraf, Khalid},
eprint = {1705.09850},
file = {:home/administrator/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Islam et al. - 2017 - Abnormality Detection and Localization in Chest X-Rays using Deep Convolutional Neural Networks(2).pdf:pdf},
month = {may},
title = {{Abnormality Detection and Localization in Chest X-Rays using Deep Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1705.09850},
year = {2017}
}
@article{Islam2017a,
abstract = {Chest X-Rays (CXRs) are widely used for diagnosing abnormalities in the heart and lung area. Automatically detecting these abnormalities with high accuracy could greatly enhance real world diagnosis processes. Lack of standard publicly available dataset and benchmark studies, however, makes it difficult to compare various detection methods. In order to overcome these difficulties, we have used a publicly available Indiana CXR, JSRT and Shenzhen dataset and studied the performance of known deep convolutional network (DCN) architectures on different abnormalities. We find that the same DCN architecture doesn't perform well across all abnormalities. Shallow features or earlier layers consistently provide higher detection accuracy compared to deep features. We have also found ensemble models to improve classification significantly compared to single model. Combining these insight, we report the highest accuracy on chest X-Ray abnormality detection on these datasets. We find that for cardiomegaly detection, the deep learning method improves the accuracy by a staggering 17 percentage point compared to rule based methods. We applied the techniques to the problem of tuberculosis detection on a different dataset and achieved the highest accuracy. Our localization experiments using these trained classifiers show that for spatially spread out abnormalities like cardiomegaly and pulmonary edema, the network can localize the abnormalities successfully most of the time. One remarkable result of the cardiomegaly localization is that the heart and its surrounding region is most responsible for cardiomegaly detection, in contrast to the rule based models where the ratio of heart and lung area is used as the measure. We believe that through deep learning based classification and localization, we will discover many more interesting features in medical image diagnosis that are not considered traditionally.},
archivePrefix = {arXiv},
arxivId = {1705.09850},
author = {Islam, Mohammad Tariqul and Aowal, Md Abdul and Minhaz, Ahmed Tahseen and Ashraf, Khalid},
eprint = {1705.09850},
file = {:home/administrator/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Islam et al. - 2017 - Abnormality Detection and Localization in Chest X-Rays using Deep Convolutional Neural Networks(2).pdf:pdf},
month = {may},
title = {{Abnormality Detection and Localization in Chest X-Rays using Deep Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1705.09850},
year = {2017}
}
@article{Haloi2018a,
abstract = {In this work, we propose advanced pneumonia and Tuberculosis grading system for X-ray images. The proposed system is a very deep fully convolutional classification network with online augmentation that outputs confidence values for diseases prevalence. Its a fully automated system capable of disease feature understanding without any offline preprocessing step or manual feature extraction. We have achieved state- of-the- art performance on the public databases such as ChestXray-14, Mendeley, Shenzhen Hospital X-ray and Belarus X-ray set.},
archivePrefix = {arXiv},
arxivId = {1807.03120},
author = {Haloi, Mrinal and Rajalakshmi, K. Raja and Walia, Pradeep},
eprint = {1807.03120},
file = {:home/administrator/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Haloi, Rajalakshmi, Walia - 2018 - Towards Radiologist-Level Accurate Deep Learning System for Pulmonary Screening(2).pdf:pdf},
month = {jun},
title = {{Towards Radiologist-Level Accurate Deep Learning System for Pulmonary Screening}},
url = {http://arxiv.org/abs/1807.03120},
year = {2018}
}
@article{Lopes2017,
abstract = {It is estimated that in 2015, approximately 1.8 million people infected by tuberculosis died, most of them in developing countries. Many of those deaths could have been prevented if the disease had been detected at an earlier stage, but the most advanced diagnosis methods are still cost prohibitive for mass adoption. One of the most popular tuberculosis diagnosis methods is the analysis of frontal thoracic radiographs; however, the impact of this method is diminished by the need for individual analysis of each radiography by properly trained radiologists. Significant research can be found on automating diagnosis by applying computational techniques to medical images, thereby eliminating the need for individual image analysis and greatly diminishing overall costs. In addition, recent improvements on deep learning accomplished excellent results classifying images on diverse domains, but its application for tuberculosis diagnosis remains limited. Thus, the focus of this work is to produce an investigation that will advance the research in the area, presenting three proposals to the application of pre-trained convolutional neural networks as feature extractors to detect the disease. The proposals presented in this work are implemented and compared to the current literature. The obtained results are competitive with published works demonstrating the potential of pre-trained convolutional networks as medical image feature extractors.},
author = {Lopes, U.K. and Valiati, J.F.},
doi = {10.1016/J.COMPBIOMED.2017.08.001},
file = {:home/administrator/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lopes, Valiati - 2017 - Pre-trained convolutional neural networks as feature extractors for tuberculosis detection.pdf:pdf},
issn = {0010-4825},
journal = {Computers in Biology and Medicine},
month = {oct},
pages = {135--143},
publisher = {Pergamon},
title = {{Pre-trained convolutional neural networks as feature extractors for tuberculosis detection}},
url = {https://www.sciencedirect.com/science/article/pii/S0010482517302548},
volume = {89},
year = {2017}
}
@article{Zhang2017,
abstract = {Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.},
archivePrefix = {arXiv},
arxivId = {1710.09412},
author = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and Lopez-Paz, David},
eprint = {1710.09412},
file = {:home/administrator/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2017 - mixup Beyond Empirical Risk Minimization(2).pdf:pdf},
month = {oct},
title = {{mixup: Beyond Empirical Risk Minimization}},
url = {http://arxiv.org/abs/1710.09412},
year = {2017}
}
@techreport{Downey2011,
author = {Downey, Allen B},
file = {:home/administrator/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Downey - 2011 - Think Stats Probability and Statistics for Programmers.pdf:pdf},
title = {{Think Stats Probability and Statistics for Programmers}},
url = {http://flickr.com/},
year = {2011}
}
@techreport{Gugger,
author = {Gugger, Sylvain},
file = {:home/administrator/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gugger - Unknown - An infinitely customizable training loop.pdf:pdf},
title = {{An infinitely customizable training loop}}
}
@misc{LukeOakden,
author = {{Luke Oakden}},
file = {:home/administrator/Documents/CheXNet{\_} an in-depth review – Luke Oakden-Rayner.pdf:pdf},
title = {{CheXNet: an in-depth review}},
url = {https://lukeoakdenrayner.wordpress.com/2018/01/24/chexnet-an-in-depth-review/}
}
@article{Zhou2018a,
abstract = {We present a weakly supervised deep learning model for classifying thoracic diseases and identifying abnormalities in chest radiography. In this work, instead of learning from medical imaging data with region-level annotations, our model was merely trained on imaging data with image-level labels to classify diseases, and is able to identify abnormal image regions simultaneously. Our model consists of a customized pooling structure and an adaptive DenseNet front-end, which can effectively recognize possible disease features for classification and localization tasks. Our method has been validated on the publicly available ChestX-ray14 dataset. Experimental results have demonstrated that our classification and localization prediction performance achieved significant improvement over the previous models on the ChestX-ray14 dataset. In summary, our network can produce accurate disease classification and localization, which can potentially support clinical decisions.},
archivePrefix = {arXiv},
arxivId = {1807.01257},
author = {Zhou, Bo and Li, Yuemeng and Wang, Jiangcong},
eprint = {1807.01257},
file = {:home/administrator/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou, Li, Wang - 2018 - A Weakly Supervised Adaptive DenseNet for Classifying Thoracic Diseases and Identifying Abnormalities.pdf:pdf},
month = {jul},
title = {{A Weakly Supervised Adaptive DenseNet for Classifying Thoracic Diseases and Identifying Abnormalities}},
url = {http://arxiv.org/abs/1807.01257},
year = {2018}
}
@article{Guendel2018,
abstract = {Chest X-ray is the most common medical imaging exam used to assess multiple pathologies. Automated algorithms and tools have the potential to support the reading workflow, improve efficiency, and reduce reading errors. With the availability of large scale data sets, several methods have been proposed to classify pathologies on chest X-ray images. However, most methods report performance based on random image based splitting, ignoring the high probability of the same patient appearing in both training and test set. In addition, most methods fail to explicitly incorporate the spatial information of abnormalities or utilize the high resolution images. We propose a novel approach based on location aware Dense Networks (DNetLoc), whereby we incorporate both high-resolution image data and spatial information for abnormality classification. We evaluate our method on the largest data set reported in the community, containing a total of 86,876 patients and 297,541 chest X-ray images. We achieve (i) the best average AUC score for published training and test splits on the single benchmarking data set (ChestX-Ray14), and (ii) improved AUC scores when the pathology location information is explicitly used. To foster future research we demonstrate the limitations of the current benchmarking setup and provide new reference patient-wise splits for the used data sets. This could support consistent and meaningful benchmarking of future methods on the largest publicly available data sets.},
annote = {Location aware},
archivePrefix = {arXiv},
arxivId = {1803.04565},
author = {Guendel, Sebastian and Grbic, Sasa and Georgescu, Bogdan and Zhou, Kevin and Ritschl, Ludwig and Meier, Andreas and Comaniciu, Dorin},
eprint = {1803.04565},
file = {:home/administrator/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guendel et al. - 2018 - Learning to recognize Abnormalities in Chest X-Rays with Location-Aware Dense Networks.pdf:pdf},
month = {mar},
title = {{Learning to recognize Abnormalities in Chest X-Rays with Location-Aware Dense Networks}},
url = {http://arxiv.org/abs/1803.04565},
year = {2018}
}
@article{Yao2017,
abstract = {The field of medical diagnostics contains a wealth of challenges which closely resemble classical machine learning problems; practical constraints, however, complicate the translation of these endpoints naively into classical architectures. Many tasks in radiology, for example, are largely problems of multi-label classification wherein medical images are interpreted to indicate multiple present or suspected pathologies. Clinical settings drive the necessity for high accuracy simultaneously across a multitude of pathological outcomes and greatly limit the utility of tools which consider only a subset. This issue is exacerbated by a general scarcity of training data and maximizes the need to extract clinically relevant features from available samples -- ideally without the use of pre-trained models which may carry forward undesirable biases from tangentially related tasks. We present and evaluate a partial solution to these constraints in using LSTMs to leverage interdependencies among target labels in predicting 14 pathologic patterns from chest x-rays and establish state of the art results on the largest publicly available chest x-ray dataset from the NIH without pre-training. Furthermore, we propose and discuss alternative evaluation metrics and their relevance in clinical practice.},
annote = {Dependencies among labels using LSTM},
archivePrefix = {arXiv},
arxivId = {1710.10501},
author = {Yao, Li and Poblenz, Eric and Dagunts, Dmitry and Covington, Ben and Bernard, Devon and Lyman, Kevin},
eprint = {1710.10501},
file = {:home/administrator/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yao et al. - 2017 - Learning to diagnose from scratch by exploiting dependencies among labels.pdf:pdf},
month = {oct},
title = {{Learning to diagnose from scratch by exploiting dependencies among labels}},
url = {http://arxiv.org/abs/1710.10501},
year = {2017}
}
@article{Wang2018b,
abstract = {Computer-aided techniques may lead to more accurate and more acces-sible diagnosis of thorax diseases on chest radiography. Despite the success of deep learning-based solutions, this task remains a major challenge in smart healthcare, since it is intrinsically a weakly supervised learning problem. In this paper, we incorporate the attention mechanism into a deep convolutional neural network, and thus propose the ChestNet model to address effective diagnosis of thorax diseases on chest radiography. This model consists of two branches: a classification branch serves as a uniform feature extraction-classification network to free users from troublesome handcrafted feature extraction, and an attention branch exploits the correlation between class labels and the locations of patholog-ical abnormalities and allows the model to concentrate adaptively on the patholog-ically abnormal regions. We evaluated our model against three state-of-the-art deep learning models on the Chest X-ray 14 dataset using the official patient-wise split. The results indicate that our model outperforms other methods, which use no extra training data, in diagnosing 14 thorax diseases on chest radiography.},
archivePrefix = {arXiv},
arxivId = {1807.03058},
author = {Wang, Hongyu and Xia, Yong},
eprint = {1807.03058},
file = {:home/administrator/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Xia - 2018 - ChestNet A Deep Neural Network for Classification of Thoracic Diseases on Chest Radiography.pdf:pdf},
month = {jul},
title = {{ChestNet: A Deep Neural Network for Classification of Thoracic Diseases on Chest Radiography}},
url = {http://arxiv.org/abs/1807.03058},
year = {2018}
}
@article{Ge2018,
abstract = {The widely used ChestX-ray14 dataset addresses an important medical image classification problem and has the following caveats: 1) many lung pathologies are visually similar, 2) a variant of diseases including lung cancer, tuberculosis, and pneumonia are present in a single scan, i.e. multiple labels and 3) The incidence of healthy images is much larger than diseased samples, creating imbalanced data. These properties are common in medical domain. Existing literature uses stateof- the-art DensetNet/Resnet models being transfer learned where output neurons of the networks are trained for individual diseases to cater for multiple diseases labels in each image. However, most of them don't consider relationship between multiple classes. In this work we have proposed a novel error function, Multi-label Softmax Loss (MSML), to specifically address the properties of multiple labels and imbalanced data. Moreover, we have designed deep network architecture based on fine-grained classification concept that incorporates MSML. We have evaluated our proposed method on various network backbones and showed consistent performance improvements of AUC-ROC scores on the ChestX-ray14 dataset. The proposed error function provides a new method to gain improved performance across wider medical datasets.},
archivePrefix = {arXiv},
arxivId = {1807.07247},
author = {Ge, Zongyuan and Mahapatra, Dwarikanath and Sedai, Suman and Garnavi, Rahil and Chakravorty, Rajib},
eprint = {1807.07247},
file = {:home/administrator/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ge et al. - 2018 - Chest X-rays Classification A Multi-Label and Fine-Grained Problem.pdf:pdf},
month = {jul},
title = {{Chest X-rays Classification: A Multi-Label and Fine-Grained Problem}},
url = {http://arxiv.org/abs/1807.07247},
year = {2018}
}
@techreport{Hsu,
abstract = {The use of diagnostic imaging has increased dramatically in recent years. A substantial number are chest x-rays used to diagnose a plethora of conditions. These diagnoses are still primarily done by radiologists manually poring over each scan, with no automated triaging or assistance. We aim to use deep learning to predict thorax disease categories using chest x-rays and their metadata with greater than first-pass specialist accuracy. Our problem can be cast as a multiclass image classification problem with 15 different labels. The paper provides a proof of concept of an automated chest x-ray diagnosis system by utilizing the NIH dataset. Deep learning is used to improve the multiclass classification accuracy of thorax disease classification , measured against a baseline of softmax regression.},
author = {Hsu, Joy and Lu, Peter and Khosla, Kush},
file = {:home/administrator/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hsu, Lu, Khosla - Unknown - Predicting Thorax Diseases with NIH Chest X-Rays.pdf:pdf},
title = {{Predicting Thorax Diseases with NIH Chest X-Rays}},
url = {https://pdfs.semanticscholar.org/6919/75f1a12148d6c30a442dd1415ff381dd25d2.pdf}
}
@article{Baltruschat2018a,
abstract = {The increased availability of X-ray image archives (e.g. the ChestX-ray14 dataset from the NIH Clinical Center) has triggered a growing interest in deep learning techniques. To provide better insight into the different approaches, and their applications to chest X-ray classification, we investigate a powerful network architecture in detail: the ResNet-50. Building on prior work in this domain, we consider transfer learning with and without fine-tuning as well as the training of a dedicated X-ray network from scratch. To leverage the high spatial resolution of X-ray data, we also include an extended ResNet-50 architecture, and a network integrating non-image data (patient age, gender and acquisition type) in the classification process. In a concluding experiment, we also investigate multiple ResNet depths (i.e. ResNet-38 and ResNet-101). In a systematic evaluation, using 5-fold re-sampling and a multi-label loss function, we compare the performance of the different approaches for pathology classification by ROC statistics and analyze differences between the classifiers using rank correlation. Overall, we observe a considerable spread in the achieved performance and conclude that the X-ray-specific ResNet-38, integrating non-image data yields the best overall results. Furthermore, class activation maps are used to understand the classification process, and a detailed analysis of the impact of non-image features is provided.},
archivePrefix = {arXiv},
arxivId = {1803.02315},
author = {Baltruschat, Ivo M. and Nickisch, Hannes and Grass, Michael and Knopp, Tobias and Saalbach, Axel},
eprint = {1803.02315},
file = {:home/administrator/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baltruschat et al. - 2018 - Comparison of Deep Learning Approaches for Multi-Label Chest X-Ray Classification.pdf:pdf},
month = {mar},
title = {{Comparison of Deep Learning Approaches for Multi-Label Chest X-Ray Classification}},
url = {http://arxiv.org/abs/1803.02315},
year = {2018}
}
@article{Yao2018a,
abstract = {Diagnostic imaging often requires the simultaneous identification of a multitude of findings of varied size and appearance. Beyond global indication of said findings, the prediction and display of localization information improves trust in and understanding of results when augmenting clinical workflow. Medical training data rarely includes more than global image-level labels as segmentations are time-consuming and expensive to collect. We introduce an approach to managing these practical constraints by applying a novel architecture which learns at multiple resolutions while generating saliency maps with weak supervision. Further, we parameterize the Log-Sum-Exp pooling function with a learnable lower-bounded adaptation (LSE-LBA) to build in a sharpness prior and better handle localizing abnormalities of different sizes using only image-level labels. Applying this approach to interpreting chest x-rays, we set the state of the art on 9 abnormalities in the NIH's CXR14 dataset while generating saliency maps with the highest resolution to date.},
archivePrefix = {arXiv},
arxivId = {1803.07703},
author = {Yao, Li and Prosky, Jordan and Poblenz, Eric and Covington, Ben and Lyman, Kevin},
eprint = {1803.07703},
file = {:home/administrator/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yao et al. - 2018 - Weakly Supervised Medical Diagnosis and Localization from Multiple Resolutions(3).pdf:pdf},
month = {mar},
title = {{Weakly Supervised Medical Diagnosis and Localization from Multiple Resolutions}},
url = {http://arxiv.org/abs/1803.07703},
year = {2018}
}
@misc{Li2018,
author = {Li, Zhe and Wang, Chong and Han, Mei and Xue, Yuan and Wei, Wei and Li, Li-Jia and Fei-Fei, Li},
file = {:home/administrator/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2018 - Thoracic Disease Identification and Localization With Limited Supervision.pdf:pdf},
pages = {8290--8299},
title = {{Thoracic Disease Identification and Localization With Limited Supervision}},
url = {http://openaccess.thecvf.com/content{\_}cvpr{\_}2018/html/Li{\_}Thoracic{\_}Disease{\_}Identification{\_}CVPR{\_}2018{\_}paper.html},
year = {2018}
}
@techreport{Wilson,
abstract = {The health care industry is expected to be an early adopter of AI and deep learning to improve patient outcomes, reduce costs, and speed up diagnosis. We have developed models for using AI to diagnose pneumonia, emphysema, and other thoracic pathologies from chest x-rays. Using the Stanford University CheXNet model as inspiration, we explore ways of developing accurate models for this problem with fast parallel training on Zenith, the Intel Xeon-based supercomputer at Dell EMC's HPC and AI Innovation Lab. We explore various network topologies to gain insight into what types of neural networks scale well in parallel and improve training time from days to hours. We then explore transferring this learned knowledge to other radiology subdomains, such as mammography, and whether this leads to better models than developing subdomain models independently.},
archivePrefix = {arXiv},
arxivId = {1609.03499},
author = {Wilson, Lucas A and Gundecha, Vineet and Varadharajan, Srinivas and Filby, Alex and Yang, Pei and Ta, Quy and Codreanu, Valeriu and Podareanu, Damian and Saletore, Vikram},
eprint = {1609.03499},
file = {:home/administrator/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wilson et al. - Unknown - Fast and Accurate Training of an AI Radiologist.pdf:pdf},
keywords = {Benchmarking,Best Practices,Distributed Training,Index Terms-Deep Learning,Medical Imaging,Radiology},
title = {{Fast and Accurate Training of an AI Radiologist}},
url = {https://sc18.supercomputing.org/proceedings/tech{\_}poster/poster{\_}files/post201s2-file3.pdf}
}
@misc{Wang2017a,
author = {Wang, Xiaosong and Peng, Yifan and Lu, Le and Lu, Zhiyong and Bagheri, Mohammadhadi and Summers, Ronald M.},
file = {:home/administrator/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2017 - ChestX-ray8 Hospital-Scale Chest X-Ray Database and Benchmarks on Weakly-Supervised Classification and Localization.pdf:pdf},
pages = {2097--2106},
title = {{ChestX-ray8: Hospital-Scale Chest X-Ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases}},
year = {2017}
}
