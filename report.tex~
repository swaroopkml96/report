\documentclass[12pt,twoside,a4paper]{report}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[style=ieee]{biblatex}
\addbibresource{references.bib}
\usepackage{graphicx}
%% Times new roman font
\usepackage{mathptmx}
%% Arial font
%\usepackage{helvet}
%\renewcommand{\familydefault}{\sfdefault}
\usepackage{hyperref}
\usepackage[all]{hypcap}
\title{Detecting abnormalities on chest X-rays using deep neural networks}
\author{
  Swaroop Kumar M L\\
  Department of Studies in Computer Science\\
  University of Mysore
}
\date{\today}
\begin{document}
\maketitle
\begin{abstract}
  Diseases like pneumonia and tuberculosis are leading causes of death
  world-wide. Although conclusive diagnosis requires other tests such as a
  sputum culture, chest radiography can be an important diagnostic aid and is
  routinely recommended since it is fast, affordable and highly sensitive.
  Moreover, automated detection of abnormalities on the chest X-ray can help in
  screening and severity-based prioritization. Due to the nature of the domain,
  it is also important that models not only make inferences
  but also generate explanations sufficient to convice a human expert.\\

  Inspired by previous work, we develop algorithms that can detect abnormalities
  on the x-ray. The algorithm explains these detections by generating heatmaps
  pointing out areas of the image that most influenced it. We establish
  baselines, benchmark against previous work and show that recent techniques
  such as mixup and progressive resizing can help to improve performance and
  generalization to other datasets. We acheive performance competitive with
  previous work in a) detecting pneumonia-like and other abnormalities on the
  NIH chestX-ray14 dataset and b) detecting tuberculosis on the Shenzhen
  hospital dataset, and achieve state-of-the-art performance on the Montgomery
  county tuberculosis dataset. We look for potential sources of bias and test
  our baseline with respect to gender, age and view position.
\end{abstract}
\tableofcontents
\chapter{Introduction}
\section{Problem definition}
The lungs are made up of small air-sacks called alveoli. When, for example, air
in the alveoli is replaced with pus, blood and other fluids, referred to as
consolidation and commonly caused by pneumonia, or when abscesses in the lung
rupture forming cavities, indicating a tuberculosis infection, these are
visible on the chest x-ray. See figure \ref{basic_examples} for examples.\\

Radiologists are trained to look for signs of these abnormalities, use subtle
visual features to differentiate among the various types, reason about their
causes and help in diagnosis and treatment. An algorithm that can automatically
detect these abnormalities can help by:
\begin{itemize}
\item{ Screening for patients with a particular disease in the general
    population }
\item{ Prioritizing patients for subsequent review by a trained radiologist }
\item{ Aiding a radiologist by being part of his/her workflow }
\end{itemize}

\begin{figure}
  % \fbox{
  % \begin{minipage}{\textwidth}
  \centering
  \includegraphics[width=0.3\textwidth]{images/normal1}\hspace{0.01\textwidth}%
  \includegraphics[width=0.3\textwidth]{images/consolidation_original}\hspace{0.01\textwidth}%
  \includegraphics[width=0.3\textwidth]{images/TB_original}\\[0.01\textwidth]
  \includegraphics[width=0.3\textwidth]{images/normal2}\hspace{0.01\textwidth}%
  \includegraphics[width=0.3\textwidth]{images/consolidation_heatmap}\hspace{0.01\textwidth}%
  \includegraphics[width=0.3\textwidth]{images/TB_heatmap}
  \caption{The first column shows two \emph{Normal} images. Columns 2 and 3 show
    images with \emph{Pneumonia} and \emph{Tuberculosis} respectively, the first
    row showing the original image and the second showing the same overlaid with
    heatmaps localizing the abnormalities, which we call \emph{explanations}}
  % \end{minipage}
  % }
  \label{basic_examples}
\end{figure}

However, building the hardware and software infrastructure for a clinically
relevant system that is useful in practice is a problem which presents unique
challenges of its own\footnote{We discuss possible implementations in section
  \ref{practical}}. Our work focuses on the core algorithm. We divide the
problem into, and explore, four sub-problems.
\subsection{Classification}
    
For our primary dataset, a large collection of chest x-rays annotated with 14
different abnormalities including pneumonia \cite{Wang2017a} (see section
\ref{nih_cxr}), we formulate the problem as a multi-class multi-label
classification problem. Given the $n$-dimensional feature space $X = \mathbb{R}
^n$, and a set of $k$ class labels corresponding to $k$ abnormalities $L =
\{\,l_1 ,l_2 ,l_3 \dots l_k\,\}$ the task is to learn a function $f:X
\rightarrow 2^L$ from the training set $D = \{\,(x_i, Y_i) \mid 1 \leq i \leq m
\,\}$. For each example $(x_i, Y_i)$, $x_i \in X$ is an $n$-dimensional feature
vector $\{\, x_{i1}, x_{i2}, x_{i3} \dots x_{in} \,\}$, each feature
representing the intensity value of a single pixel of the input image, and $Y_i
\subseteq L $ is the set of abnormalities associated with $x_i$. For an unseen
instance $x$, the classifier $f(\cdot)$ predicts $f(x) \subseteq L$ as the set of
abnormalities for
$x$.\\

For the Shenzhen hospital tuberculosis dataset (see section \ref{szhenzhen}), we
formulate the problem as a binary classification problem. Again, suppose $X$ is
the $n$-dimensional feature space. The task is to learn a function $f:X
\rightarrow \{\,0, 1 \,\}$ from the training set $D = \{\,(\,x_i, y_i) \mid 1
\leq i \leq m\,\}$. For each $(x_i, y_i) \in D$, $x_i = \{\,x_{i1}, x_{i2},
x_{i3} \dots x_{in}\,\}$ is an $n$-dimensional feature vector, each feature
corresponding to the intensity value of a single pixel of the input image, and
$y_i \in \{\,0, 1\,\}$ is the corresponding label, $0$ meaning \emph{Normal} and
$1$ meaning \emph{Tuberculosis}. Given an unseen $x \in X$, the classifier
$f(\cdot)$ predicts $y \in \{\,0, 1\,\}$ as being the label for $x$.\\

In both cases, $f(\cdot)$ is a deep convolutional neural
network\cite{lecun1998gradient}, specifically a variant of
densenet\cite{iandola2014densenet} trained using the Adam\cite{kingma2014adam}
optimization algorithm (see sections \ref{architecture} and \ref{training_procedure} for 
discussions about the model architecture and training procedure). 
In the case of NIH ChestX-ray14, our primary dataset, the output
of the network is a 14-dimensional vector $P = \{\,p_1, p_2, p_3 \dots p_{14}\,\}$ where
$p_k \in P$ corresponds to the $k^{th}$ abnormality and $0 \leq p_k \leq 1$. A set of optimal
thresholds $T = \{\,t_1, t_2, t_3 \dots t_{14}\,\}$ is determined from the training set by
maximizing the F1-score and applied to the output of the network so that the final output $Y \subseteq L$ is:
\begin{equation}
Y = \{\,l_i \in L \mid p_i \in P \wedge t_i \in T \wedge p_i > t_i \,\}
\end{equation}
In the case of the Shenzhen hospital tuberculosis dataset, the output of the network
is a 2-dimensional vector $P = \{\,p_1, p_2\,\}$ where $0 \leq p_1 \leq 1$ and $0 \leq p_2 \leq 1$
and the final output $y$ is:
\begin{equation}
y = 
\begin{cases}
0 & \text{if $p1 \geq p2$\, (Normal)}\\
1 & \text{if $p2 > p1$\, (Tuberculosis)}
\end{cases}
\end{equation}

\subsection{Explainability}
Deep neural networks have outperformed previous methods in several domains. However, 
they remain black-boxes with millions of parameters, limiting their use
in routine clinical practice. Several methods have been proposed to 

\subsection{Generalizability}

\subsection{Fairness}

\section{Motivation}
Here I talk about the social/economic aspects of these diseases, compare
different diagnosis methods and how and why chest radiography, and our work in
particular can help. Also, how this can be part of a bigger practical
implementation integrated into existing radiologist workflow. More detail on the
latter in section \ref{practical}.
\section{Previous work}
A breif overview of previous work. A more detailed description will be in
chapter \ref{litsurvey}, \emph{Literature survey}.
\section{Our work}
Short overview of our experiments and results. A condensed version of chapters
\ref{exp}, \ref{res} and \ref{bias}
\section{Report layout}
Overall layout of the rest of the report.
\chapter{Literature survey\label{litsurvey}}
\section{CheXNet and CheXNext}
\section{Weakly supervised learning}
The body of work around weakly supervised learning, and for our purposes, two
forms of it:
\begin{enumerate}
\item{Learning with inaccurate labels. For example, learning from labels which
    were generated algorithmically and which may therefore be inaccurate. Here,
    labels were extracted from radiology reports in natural language text.}
\item{Learning from imprecise labels. For example, learning to precisely
    localize objects or patterns with imprecise image-level labels. We use this
    to generate explanations.}
\end{enumerate}
\section{Explainability}
A short review of methods such as weakly supervised localization, LIME and SHAP
which have been developed for generating explanations.
\section{Fairness}
Methods to quantify learned bias along the lines of gender, race, etc.
\section{Learning at multiple scales}
Methods to effectively combine inferences from multiple scales.
\section{Attention}
Methods to allow models to selectively pay attention to parts of an image.
\section{Recurrent neural networls}
A number of papers have shown that using recurrent neural networks to
effectively make use of correlations between different abnormalities improves
performance.
\section{Generalizability}
Atleast one other paper has studied how models in this domain generalize to
other datasets. I describe their results.
\section{Other methods}
\section{Tuberculosis}
\chapter{Data}
Here I describe all the datasets we use, as well as how we split them, and where
and how we use k-fold cross validation
\section{NIH CXR-14\label{nih_cxr}}
The NIH chestX-ray 14 dataset, our primary dataset annotated with 14 different
abnormalities including pneumonia.
\subsection{Challenges and issues}
\section{Mendeley\label{mendeley}}
This is the Mendelay pneumonia dataset of CXRs of children under 5 years, which
we use to test generalization.
\section{Szhenzhen\label{szhenzhen}}
Szhenzhen and Montgomery county tuberculosis datasets. Shenzhen is used to train
models and Montgomery is used to test generalization.
\section{Montgomery\label{montgomery}}
\chapter{Baselines}
\section{Model architecture\label{architecture}}
\section{Training procedure\label{training_procedure}}
\section{Weakly supervised localization}
\subsection{Saliency map to bounding box}
\subsection{LIME as an alternative}
\chapter{Experiments\label{exp}}
\section{Data augmentation}
\section{Test-time augmentation}
\section{Higher resolution}
\section{Progressive resizing}
\section{Ensembling for scale-invariance}
\section{Ensembling saliency maps}
\section{Mixup}
\section{Self-training}
\section{Transfer learning for tuberculosis}
\section{Generalizability}
\chapter{Results\label{res}}
\section{Comparison metrics}
\section{Comparison to previous work}
\section{Comparison to human radiologists}
\subsection{Caveats with comparison to human radiologists}
\section{Other important factors}
Such as how well predicted probabilities correspond to actual severity,
localization, and time
\section{Examples}
\chapter{Bias\label{bias}}
\section{Gender}
\section{Age}
\section{View position}
\chapter{Conclusion}
\chapter{Future work}
\section{Limitations of proposed work}
\section{Avenues for further research}
\section{Practical implementation and clinical relevance\label{practical}}
\section{More recent datasets}
Such as CheXPert and PadChest \printbibliography
\end{document}
